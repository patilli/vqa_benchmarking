:mod:`vqa_benchmarking_backend.bottomupattention.evaluation`
============================================================

.. py:module:: vqa_benchmarking_backend.bottomupattention.evaluation


Submodules
----------
.. toctree::
   :titlesonly:
   :maxdepth: 1

   vg_eval/index.rst
   vg_evaluation/index.rst


Package Contents
----------------

Classes
~~~~~~~

.. autoapisummary::

   vqa_benchmarking_backend.bottomupattention.evaluation.VGEvaluator




.. class:: VGEvaluator(dataset_name, cfg, distributed, output_dir=None)


   Bases: :py:obj:`detectron2.evaluation.evaluator.DatasetEvaluator`

   Evaluate object proposal, instance detection
   outputs using VG's metrics and APIs.

   .. method:: _tasks_from_config(self, cfg)

      Returns:
      tuple[str]: tasks that can be evaluated under the given configuration.


   .. method:: gt_roidb(self, dataset)


   .. method:: reset(self)


   .. method:: process(self, inputs, outputs)

      Args:
      inputs: the inputs to a COCO model (e.g., GeneralizedRCNN).
          It is a list of dict. Each dict corresponds to an image and
          contains keys like "height", "width", "file_name", "image_id".
      outputs: the outputs of a COCO model. It is a list of dicts with key
          "instances" that contains :class:`Instances`.


   .. method:: evaluate(self)


   .. method:: _eval_vg(self)


   .. method:: write_voc_results_file(self, predictions, output_dir)


   .. method:: get_vg_results_file_template(self, output_dir, pickle=True, eval_attributes=False)


   .. method:: do_python_eval(self, output_dir, pickle=True, eval_attributes=False)



