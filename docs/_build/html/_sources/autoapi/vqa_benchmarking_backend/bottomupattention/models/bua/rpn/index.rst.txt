:mod:`vqa_benchmarking_backend.bottomupattention.models.bua.rpn`
================================================================

.. py:module:: vqa_benchmarking_backend.bottomupattention.models.bua.rpn


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   vqa_benchmarking_backend.bottomupattention.models.bua.rpn.StandardBUARPNHead
   vqa_benchmarking_backend.bottomupattention.models.bua.rpn.BUARPN




.. class:: StandardBUARPNHead(cfg, input_shape: List[detectron2.layers.ShapeSpec])


   Bases: :py:obj:`torch.nn.Module`

   RPN classification and regression heads. Uses a 3x3 conv to produce a shared
   hidden state from which one 1x1 conv predicts objectness logits for each anchor
   and a second 1x1 conv predicts bounding-box deltas specifying how to deform
   each anchor into an object proposal.

   .. method:: forward(self, features)

      Args:
      features (list[Tensor]): list of feature maps



.. class:: BUARPN(cfg, input_shape: Dict[str, detectron2.layers.ShapeSpec])


   Bases: :py:obj:`torch.nn.Module`

   Region Proposal Network, introduced by the Faster R-CNN paper.

   .. method:: forward(self, images, features, gt_instances=None)

      Args:
          images (ImageList): input images of length `N`
          features (dict[str: Tensor]): input data as a mapping from feature
              map name to tensor. Axis 0 represents the number of images `N` in
              the input data; axes 1-3 are channels, height, and width, which may
              vary between feature maps (e.g., if a feature pyramid is used).
          gt_instances (list[Instances], optional): a length `N` list of `Instances`s.
              Each `Instances` stores ground-truth instances for the corresponding image.

      Returns:
          proposals: list[Instances] or None
          loss: dict[Tensor]



