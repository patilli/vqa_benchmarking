:mod:`vqa_benchmarking_backend.bottomupattention.models.bua`
============================================================

.. py:module:: vqa_benchmarking_backend.bottomupattention.models.bua


Submodules
----------
.. toctree::
   :titlesonly:
   :maxdepth: 1

   backbone/index.rst
   box_regression/index.rst
   config/index.rst
   fast_rcnn/index.rst
   postprocessing/index.rst
   rcnn/index.rst
   roi_heads/index.rst
   rpn/index.rst
   rpn_outputs/index.rst


Package Contents
----------------

Classes
~~~~~~~

.. autoapisummary::

   vqa_benchmarking_backend.bottomupattention.models.bua.GeneralizedBUARCNN
   vqa_benchmarking_backend.bottomupattention.models.bua.BUACaffeRes5ROIHeads
   vqa_benchmarking_backend.bottomupattention.models.bua.StandardBUARPNHead
   vqa_benchmarking_backend.bottomupattention.models.bua.BUARPN



Functions
~~~~~~~~~

.. autoapisummary::

   vqa_benchmarking_backend.bottomupattention.models.bua.add_bottom_up_attention_config
   vqa_benchmarking_backend.bottomupattention.models.bua.build_bua_resnet_backbone



.. function:: add_bottom_up_attention_config(cfg, caffe=False)

   Add config for tridentnet.


.. function:: build_bua_resnet_backbone(cfg, input_shape)

   Create a ResNet instance from config.

   Returns:
       ResNet: a :class:`ResNet` instance.


.. class:: GeneralizedBUARCNN(cfg)


   Bases: :py:obj:`torch.nn.Module`

   Generalized R-CNN. Any models that contains the following three components:
   1. Per-image feature extraction (aka backbone)
   2. Region proposal generation
   3. Per-region feature extraction and prediction

   .. method:: forward(self, batched_inputs)

      Args:
          batched_inputs: a list, batched outputs of :class:`DatasetMapper` .
              Each item in the list contains the inputs for one image.
              For now, each item in the list is a dict that contains:

              * image: Tensor, image in (C, H, W) format.
              * instances (optional): groundtruth :class:`Instances`
              * proposals (optional): :class:`Instances`, precomputed proposals.

              Other information that's included in the original dicts, such as:

              * "height", "width" (int): the output resolution of the model, used in inference.
                  See :meth:`postprocess` for details.

      Returns:
          list[dict]:
              Each dict is the output for one input image.
              The dict contains one key "instances" whose value is a :class:`Instances`.
              The :class:`Instances` object has the following keys:
                  "pred_boxes", "pred_classes", "scores", "pred_masks", "pred_keypoints"


   .. method:: inference(self, batched_inputs, detected_instances=None, do_postprocess=True)

      Run inference on the given inputs.

      Args:
          batched_inputs (list[dict]): same as in :meth:`forward`
          detected_instances (None or list[Instances]): if not None, it
              contains an `Instances` object per image. The `Instances`
              object contains "pred_boxes" and "pred_classes" which are
              known boxes in the image.
              The inference will then skip the detection of bounding boxes,
              and only predict other per-ROI outputs.
          do_postprocess (bool): whether to apply post-processing on the outputs.

      Returns:
          same as in :meth:`forward`.


   .. method:: preprocess_image(self, batched_inputs)

      Normalize, pad and batch the input images.



.. class:: BUACaffeRes5ROIHeads(cfg, input_shape)


   Bases: :py:obj:`detectron2.modeling.ROIHeads`

   The ROIHeads in a typical "C4" R-CNN model, where
   the box and mask head share the cropping and
   the per-region feature computation by a Res5 block.

   .. method:: _build_res5_block(self, cfg)


   .. method:: _shared_roi_transform(self, features, boxes)


   .. method:: forward(self, images, features, proposals, targets=None)

      See :class:`ROIHeads.forward`.



.. class:: StandardBUARPNHead(cfg, input_shape: List[detectron2.layers.ShapeSpec])


   Bases: :py:obj:`torch.nn.Module`

   RPN classification and regression heads. Uses a 3x3 conv to produce a shared
   hidden state from which one 1x1 conv predicts objectness logits for each anchor
   and a second 1x1 conv predicts bounding-box deltas specifying how to deform
   each anchor into an object proposal.

   .. method:: forward(self, features)

      Args:
      features (list[Tensor]): list of feature maps



.. class:: BUARPN(cfg, input_shape: Dict[str, detectron2.layers.ShapeSpec])


   Bases: :py:obj:`torch.nn.Module`

   Region Proposal Network, introduced by the Faster R-CNN paper.

   .. method:: forward(self, images, features, gt_instances=None)

      Args:
          images (ImageList): input images of length `N`
          features (dict[str: Tensor]): input data as a mapping from feature
              map name to tensor. Axis 0 represents the number of images `N` in
              the input data; axes 1-3 are channels, height, and width, which may
              vary between feature maps (e.g., if a feature pyramid is used).
          gt_instances (list[Instances], optional): a length `N` list of `Instances`s.
              Each `Instances` stores ground-truth instances for the corresponding image.

      Returns:
          proposals: list[Instances] or None
          loss: dict[Tensor]



